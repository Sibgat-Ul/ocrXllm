{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-18T14:27:46.785152Z",
     "iopub.status.busy": "2025-12-18T14:27:46.784524Z",
     "iopub.status.idle": "2025-12-18T14:29:21.969559Z",
     "shell.execute_reply": "2025-12-18T14:29:21.968664Z",
     "shell.execute_reply.started": "2025-12-18T14:27:46.785129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "!pip install bitsandbytes -q\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import M2M100ForConditionalGeneration, AutoTokenizer, SiglipImageProcessor\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from huggingface_hub import snapshot_download, login\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# model_name = \"mtmlt/sonar-nllb-200-1.3B\"\n",
    "# model_dir = snapshot_download(model_name)\n",
    "\n",
    "# model = M2M100ForConditionalGeneration.from_pretrained(model_dir)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# def encode_mean_pool(texts, tokenizer, encoder, lang='eng_Latn', norm=False):\n",
    "#     tokenizer.src_lang = lang\n",
    "#     with torch.inference_mode():\n",
    "#         batch = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "#         seq_embs = encoder(**batch)\n",
    "#         mask = batch.attention_mask\n",
    "#         mean_emb = (seq_embs.last_hidden_state * mask.unsqueeze(-1)).sum(1) / mask.unsqueeze(-1).sum(1)\n",
    "#         if norm:\n",
    "#             mean_emb = torch.nn.functional.normalize(mean_emb)\n",
    "#     return mean_emb, seq_embs, mask\n",
    "\n",
    "# sentences = ['My name is SONAR.', 'I can embed the sentences into vector space.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:29:21.971501Z",
     "iopub.status.busy": "2025-12-18T14:29:21.970854Z",
     "iopub.status.idle": "2025-12-18T14:30:02.844504Z",
     "shell.execute_reply": "2025-12-18T14:30:02.843851Z",
     "shell.execute_reply.started": "2025-12-18T14:29:21.971482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"mtmlt/sonar-nllb-200-1.3B\"\n",
    "model_dir = snapshot_download(model_name)\n",
    "img_model_dir = snapshot_download(\"google/siglip-base-patch16-384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:30:02.845555Z",
     "iopub.status.busy": "2025-12-18T14:30:02.845216Z",
     "iopub.status.idle": "2025-12-18T14:30:05.189140Z",
     "shell.execute_reply": "2025-12-18T14:30:05.188424Z",
     "shell.execute_reply.started": "2025-12-18T14:30:02.845537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "decoder_model = M2M100ForConditionalGeneration.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.tgt_lang = 'eng_Latn'\n",
    "tokenizer.src_lang = 'eng_Latn'\n",
    "decoder_model.eval()  \n",
    "\n",
    "decoder_encoder = decoder_model.model.encoder  \n",
    "decoder_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:30:05.191301Z",
     "iopub.status.busy": "2025-12-18T14:30:05.191059Z",
     "iopub.status.idle": "2025-12-18T14:30:05.764718Z",
     "shell.execute_reply": "2025-12-18T14:30:05.764107Z",
     "shell.execute_reply.started": "2025-12-18T14:30:05.191262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import SiglipVisionModel, get_cosine_schedule_with_warmup\n",
    "\n",
    "class SonarImageEnc(nn.Module):\n",
    "    def __init__(self, path=\"google/siglip-base-patch16-384\", sonar_dim=1024):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = SiglipVisionModel.from_pretrained(path, torch_dtype=\"auto\")\n",
    "        for p in self.vision_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        vdim = self.vision_encoder.config.hidden_size\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(vdim, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, sonar_dim)\n",
    "            \n",
    "        )\n",
    "        self.norm = nn.LayerNorm(sonar_dim)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        h = self.vision_encoder(pixel_values).last_hidden_state\n",
    "        h = self.proj(h)\n",
    "        h = h.mean(dim=1) \n",
    "        return self.norm(h) \n",
    "\n",
    "image_encoder = SonarImageEnc(path=img_model_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:30:05.766240Z",
     "iopub.status.busy": "2025-12-18T14:30:05.766036Z",
     "iopub.status.idle": "2025-12-18T14:30:09.664653Z",
     "shell.execute_reply": "2025-12-18T14:30:09.663761Z",
     "shell.execute_reply.started": "2025-12-18T14:30:05.766216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_to_sonar_embedding(texts=None, tokenizer=None, encoder_model=None, batch=None, lang='eng_Latn', device='cpu', norm=False):\n",
    "    if batch is None:\n",
    "        tokenizer.src_lang = lang\n",
    "        batch = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    encoder_model.to(device)\n",
    "    with torch.no_grad():\n",
    "        seq_embs = encoder_model(**batch)\n",
    "    mask = batch['attention_mask']\n",
    "    mean_emb = (seq_embs.last_hidden_state * mask.unsqueeze(-1)).sum(1) / mask.sum(1, keepdim=True)\n",
    "    if norm:\n",
    "        mean_emb = F.normalize(mean_emb, dim=-1)\n",
    "    return mean_emb, seq_embs, mask, batch\n",
    "\n",
    "for param in decoder_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:30:09.665994Z",
     "iopub.status.busy": "2025-12-18T14:30:09.665648Z",
     "iopub.status.idle": "2025-12-18T14:32:42.351058Z",
     "shell.execute_reply": "2025-12-18T14:32:42.350320Z",
     "shell.execute_reply.started": "2025-12-18T14:30:09.665969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# dset = snapshot_download(\"pixparse/cc12m-wds\", allow_patterns=[\"cc12m-train-0000.tar\"], repo_type=\"dataset\")\n",
    "\n",
    "# dset = snapshot_download(\"pixparse/cc12m-wds\", allow_patterns=[\"cc12m-train-000[0-9].tar\"], repo_type=\"dataset\")\n",
    "# dset = snapshot_download(\"pixparse/cc12m-wds\", allow_patterns=[\"cc12m-train-001[0-9].tar\"], repo_type=\"dataset\")\n",
    "# dset = snapshot_download(\"pixparse/cc12m-wds\", allow_patterns=[\"cc12m-train-002[0-9].tar\"], repo_type=\"dataset\")\n",
    "# dset = snapshot_download(\"pixparse/cc12m-wds\", allow_patterns=[\"cc12m-train-003[0-9].tar\"], repo_type=\"dataset\")\n",
    "# dset = snapshot_download(\"pixparse/cc12m-wds\", allow_patterns=[\"cc12m-train-004[0-9].tar\"], repo_type=\"dataset\")\n",
    "# dset = snapshot_download(\"pixparse/cc12m-wds\", allow_patterns=[\"cc12m-train-005[0-9].tar\"], repo_type=\"dataset\")\n",
    "\n",
    "dset = snapshot_download(\"romrawinjp/multilingual-coco\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:32:42.352253Z",
     "iopub.status.busy": "2025-12-18T14:32:42.351984Z",
     "iopub.status.idle": "2025-12-18T14:32:42.621369Z",
     "shell.execute_reply": "2025-12-18T14:32:42.620705Z",
     "shell.execute_reply.started": "2025-12-18T14:32:42.352229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(dset, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:32:42.622397Z",
     "iopub.status.busy": "2025-12-18T14:32:42.622060Z",
     "iopub.status.idle": "2025-12-18T14:32:49.785562Z",
     "shell.execute_reply": "2025-12-18T14:32:49.784897Z",
     "shell.execute_reply.started": "2025-12-18T14:32:42.622377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_processor = SiglipImageProcessor.from_pretrained(img_model_dir)  \n",
    "\n",
    "def normalize_caption(cap_item):\n",
    "    \"\"\"\n",
    "    Extract first valid string from nested tuple/list structure.\n",
    "    Returns the string or None if invalid.\n",
    "    \"\"\"\n",
    "    if isinstance(cap_item, (tuple, list)):\n",
    "        for elem in cap_item:\n",
    "            if isinstance(elem, str) and elem.strip():\n",
    "                return elem\n",
    "        return None\n",
    "    elif isinstance(cap_item, str) and cap_item.strip():\n",
    "        return cap_item\n",
    "    return None\n",
    "\n",
    "def process_img(batch):    \n",
    "    lang_to_sonar = {\n",
    "        \"en\": \"eng_Latn\",\n",
    "        \"ar\": \"arb_Arab\",\n",
    "        \"de\": \"dan_Latn\",\n",
    "        \"ru\": \"rus_Cyrl\"\n",
    "    }\n",
    "    \n",
    "    rgb_images = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
    "    processed_img = image_processor(images=rgb_images, return_tensors=\"pt\")\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": processed_img[\"pixel_values\"],\n",
    "        \"eng_Latn\": [normalize_caption(cap) for cap in batch[\"en\"]],\n",
    "        \"arb_Arab\": [normalize_caption(cap) for cap in batch[\"ar\"]],\n",
    "        \"dan_Latn\": [normalize_caption(cap) for cap in batch[\"de\"]],\n",
    "        \"rus_Cyrl\": [normalize_caption(cap) for cap in batch[\"ru\"]]\n",
    "    }\n",
    "    \n",
    "def process_img_val(batch):    \n",
    "    rgb_images = [img.convert(\"RGB\") for img in batch[\"jpg\"]]\n",
    "    processed_img = image_processor(images=rgb_images, return_tensors=\"pt\")\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": processed_img[\"pixel_values\"],\n",
    "        \"caption\": batch[\"txt\"]\n",
    "    }\n",
    "\n",
    "import os as _os\n",
    "_hf_token = _os.getenv(\"HF_TOKEN\")\n",
    "if _hf_token:\n",
    "    login(_hf_token)\n",
    "else:\n",
    "    print(\"HF_TOKEN not set; proceeding without login.\")\n",
    "\n",
    "dataset_train = load_dataset(dset, streaming=True)\n",
    "dataset_train = dataset_train.map(process_img, batched=True, batch_size=32, remove_columns=list(next(iter(dataset_train.values())).features)).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:32:49.786516Z",
     "iopub.status.busy": "2025-12-18T14:32:49.786288Z",
     "iopub.status.idle": "2025-12-18T14:33:32.285635Z",
     "shell.execute_reply": "2025-12-18T14:33:32.285052Z",
     "shell.execute_reply.started": "2025-12-18T14:32:49.786499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_hf_dataset = load_dataset(\"pixparse/cc12m-wds\", data_files=\"cc12m-train-0999.tar\")\n",
    "val_hf_dataset = val_hf_dataset[\"train\"]\n",
    "val_hf_dataset = val_hf_dataset.select(range(0, 1000))\n",
    "val_hf_dataset = val_hf_dataset.map(process_img_val, batched=True, batch_size=128, remove_columns=val_hf_dataset.column_names, keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:33:32.287923Z",
     "iopub.status.busy": "2025-12-18T14:33:32.287715Z",
     "iopub.status.idle": "2025-12-18T14:33:32.292729Z",
     "shell.execute_reply": "2025-12-18T14:33:32.292098Z",
     "shell.execute_reply.started": "2025-12-18T14:33:32.287906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_hf_dataset = dataset_train[\"train\"]\n",
    "batch_size = 3\n",
    "train_loader = DataLoader(train_hf_dataset, batch_size=batch_size)\n",
    "\n",
    "val_hf_dataset.set_format(\"torch\")\n",
    "val_loader = DataLoader(val_hf_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:33:32.293507Z",
     "iopub.status.busy": "2025-12-18T14:33:32.293338Z",
     "iopub.status.idle": "2025-12-18T14:33:32.310611Z",
     "shell.execute_reply": "2025-12-18T14:33:32.310023Z",
     "shell.execute_reply.started": "2025-12-18T14:33:32.293493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "save_dir = \"./adapter_checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:33:32.311511Z",
     "iopub.status.busy": "2025-12-18T14:33:32.311208Z",
     "iopub.status.idle": "2025-12-18T14:33:32.717050Z",
     "shell.execute_reply": "2025-12-18T14:33:32.716302Z",
     "shell.execute_reply.started": "2025-12-18T14:33:32.311488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:33:32.718167Z",
     "iopub.status.busy": "2025-12-18T14:33:32.717862Z",
     "iopub.status.idle": "2025-12-18T14:33:36.150344Z",
     "shell.execute_reply": "2025-12-18T14:33:36.149466Z",
     "shell.execute_reply.started": "2025-12-18T14:33:32.718142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import bitsandbytes as bnb\n",
    "from torch.amp import GradScaler\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "def validate(\n",
    "    image_encoder,\n",
    "    decoder_model,\n",
    "    val_loader,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_batches=None,\n",
    "    max_new_tokens=48,\n",
    "    num_beams=5\n",
    "):\n",
    "    image_encoder.eval()\n",
    "    decoder_model.eval()\n",
    "    \n",
    "    total_ce_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            if max_batches and i >= max_batches:\n",
    "                break\n",
    "\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            captions = batch['caption']  \n",
    "\n",
    "            tokenizer.src_lang = 'eng_Latn'\n",
    "            tokenized_en = tokenizer(\n",
    "                captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
    "            ).to(device)\n",
    "            \n",
    "            labels = tokenized_en['input_ids'].clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            img_vec = image_encoder(pixel_values)         \n",
    "            # img_vec = F.normalize(img_vec, dim=-1)        \n",
    "            enc_out = BaseModelOutput(last_hidden_state=img_vec.unsqueeze(1)) \n",
    "\n",
    "            enc_mask = torch.ones(img_vec.size(0), 1, dtype=torch.long, device=device)\n",
    "\n",
    "            outputs_en = decoder_model(\n",
    "                encoder_outputs=enc_out,\n",
    "                attention_mask=enc_mask,\n",
    "                labels=labels,\n",
    "                return_dict=True\n",
    "            )\n",
    "            ce_loss = outputs_en.loss\n",
    "            total_ce_loss += ce_loss.item()\n",
    "            \n",
    "            generated_en = decoder_model.generate(\n",
    "                encoder_outputs=BaseModelOutput(last_hidden_state=img_vec.unsqueeze(1)),\n",
    "                forced_bos_token_id=tokenizer.convert_tokens_to_ids('eng_Latn'),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_beams=num_beams,\n",
    "            )\n",
    "            preds_en = tokenizer.batch_decode(generated_en, skip_special_tokens=True)\n",
    "\n",
    "            tokenizer.src_lang = 'ben_Beng'\n",
    "            generated_bn = decoder_model.generate(\n",
    "                encoder_outputs=BaseModelOutput(last_hidden_state=img_vec.unsqueeze(1)),\n",
    "                forced_bos_token_id=tokenizer.convert_tokens_to_ids('ben_Beng'),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_beams=num_beams,\n",
    "            )\n",
    "            preds_bn = tokenizer.batch_decode(generated_bn, skip_special_tokens=True)\n",
    "            tokenizer.src_lang = 'eng_Latn'  \n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"Batch {i} | Zero-Shot Bengali Inspection\")\n",
    "                print(f\"{'='*70}\")\n",
    "                print(f\"True EN:       {captions[:2]}\")\n",
    "                print(f\"Generated EN:  {preds_en[:2]}\")\n",
    "                print(f\"Generated BEN: {preds_bn[:2]}\")\n",
    "                print(f\"CE Loss: {ce_loss.item():.3f} | IMG vec std: {img_vec.std():.4f}\")\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_ce = total_ce_loss / num_batches if num_batches > 0 else 0\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"VALIDATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total batches: {num_batches}\")\n",
    "    print(f\"Avg CE Loss: {avg_ce:.4f} (lower is better)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    image_encoder.train()\n",
    "    decoder_model.train()\n",
    "    \n",
    "    return avg_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:33:36.151633Z",
     "iopub.status.busy": "2025-12-18T14:33:36.151382Z",
     "iopub.status.idle": "2025-12-18T14:33:36.162993Z",
     "shell.execute_reply": "2025-12-18T14:33:36.162237Z",
     "shell.execute_reply.started": "2025-12-18T14:33:36.151613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_image_embed(\n",
    "        image_encoder, \n",
    "        decoder_model, \n",
    "        pixel_values, \n",
    "        labels, \n",
    "        normalize=False, \n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "\n",
    "    img_vec = image_encoder(pixel_values) \n",
    "\n",
    "    if normalize:\n",
    "        img_vec = F.normalize(img_vec, dim=-1)\n",
    "\n",
    "    enc_out = BaseModelOutput(last_hidden_state=img_vec.unsqueeze(1))  \n",
    "\n",
    "    enc_mask = torch.ones(img_vec.size(0), 1, dtype=torch.long, device=device)\n",
    "\n",
    "    outputs = decoder_model(\n",
    "        encoder_outputs=enc_out,\n",
    "        attention_mask=enc_mask,         \n",
    "        labels=labels,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    return outputs, img_vec\n",
    "\n",
    "def generate_text_embed(\n",
    "    tokenized,\n",
    "    encoder_model=None,\n",
    "    decoder_model=None,\n",
    "):\n",
    "    seq_embs = encoder_model(**tokenized)\n",
    "    mask = tokenized['attention_mask']\n",
    "    text_vec = (seq_embs.last_hidden_state * mask.unsqueeze(-1)).sum(1) / mask.sum(1, keepdim=True)\n",
    "    \n",
    "    text_out = BaseModelOutput(last_hidden_state=text_vec.unsqueeze(1))\n",
    "    text_mask = torch.ones(text_vec.size(0), 1, dtype=torch.long, device=text_vec.device)\n",
    "    outputs = decoder_model(\n",
    "        encoder_outputs=text_out,\n",
    "        attention_mask=text_mask,         \n",
    "        labels=tokenized['input_ids'],\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    return outputs, text_vec\n",
    "\n",
    "def train_multilingual(\n",
    "    image_encoder,\n",
    "    decoder_model,\n",
    "    tokenizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device='cuda',\n",
    "    max_steps=12000,\n",
    "    grad_accum=48,\n",
    "    lr=2e-4,\n",
    "    eval_every=1000,\n",
    "    save_dir='./checkpoints',\n",
    "    log_every=500,\n",
    "    max_new_tokens=48,\n",
    "    num_beams=5\n",
    "):\n",
    "    image_encoder.train().to(device)\n",
    "    decoder_model.eval().to(device)\n",
    "    text_enc = decoder_model.model.encoder\n",
    "    for p in decoder_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in text_enc.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    langs = [\"eng_Latn\"]\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(\n",
    "        filter(lambda p: p.requires_grad, image_encoder.parameters()),\n",
    "        lr=lr, weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=500, num_training_steps=max_steps\n",
    "    )\n",
    "\n",
    "    global_step, best_val = 0, float('inf')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    while global_step < max_steps:\n",
    "        epoch = (global_step // 82800) + 1\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep {epoch}\")\n",
    "\n",
    "        for micro_step, batch in enumerate(pbar):\n",
    "            pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n",
    "            captions, lang = None, None\n",
    "            for l in langs:\n",
    "                if batch.get(l):\n",
    "                    captions, lang = batch[l], l\n",
    "                    break\n",
    "            if captions is None:\n",
    "                continue\n",
    "\n",
    "            tokenizer.src_lang = lang\n",
    "            tok = tokenizer(captions, return_tensors='pt', padding=True,\n",
    "                            truncation=True, max_length=128).to(device)\n",
    "            labels = tok.input_ids.clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            outputs_text, text_vec = generate_text_embed(\n",
    "                tokenized=tok,\n",
    "                encoder_model=text_enc,\n",
    "                decoder_model=decoder_model,\n",
    "            )\n",
    "            \n",
    "            outputs, img_vec = generate_image_embed(\n",
    "                image_encoder,\n",
    "                decoder_model,\n",
    "                pixel_values,\n",
    "                labels,\n",
    "                normalize=False,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            loss_mse = F.mse_loss(img_vec, text_vec)\n",
    "\n",
    "            loss = (outputs.loss + loss_mse + 0.5*outputs_text.loss) / grad_accum\n",
    "\n",
    "            loss.backward()\n",
    "            if (micro_step + 1) % grad_accum == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(image_encoder.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                global_step += 1\n",
    "\n",
    "                pbar.set_postfix({'loss': loss.item() * grad_accum, \n",
    "                                  'loss_mse': loss_mse.item(), \n",
    "                                  'loss_text': outputs_text.loss.item(),\n",
    "                                  'loss_img': outputs.loss.item(),\n",
    "                                  'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "\n",
    "                # Log training captions every log_every steps\n",
    "                if global_step % log_every == 0:\n",
    "                    print(f\"loss_mse: {loss_mse.item():.3f}, loss_text: {outputs_text.loss.item():.3f}, loss_img: {outputs.loss.item():.3f}\")\n",
    "                    image_encoder.eval()\n",
    "                    with torch.no_grad():\n",
    "                        img_vec_eval = image_encoder(pixel_values)\n",
    "                        enc_out = BaseModelOutput(last_hidden_state=img_vec_eval.unsqueeze(1))\n",
    "                        enc_mask = torch.ones(img_vec_eval.size(0), 1, dtype=torch.long, device=device)\n",
    "                        \n",
    "                        # Generate English captions\n",
    "                        generated_en = decoder_model.generate(\n",
    "                            encoder_outputs=enc_out,\n",
    "                            attention_mask=enc_mask,\n",
    "                            forced_bos_token_id=tokenizer.convert_tokens_to_ids('eng_Latn'),\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            num_beams=num_beams,\n",
    "                        )\n",
    "                        preds_en = tokenizer.batch_decode(generated_en, skip_special_tokens=True)\n",
    "                        \n",
    "                        # Generate Bengali captions (zero-shot)\n",
    "                        generated_bn = decoder_model.generate(\n",
    "                            encoder_outputs=enc_out,\n",
    "                            attention_mask=enc_mask,\n",
    "                            forced_bos_token_id=tokenizer.convert_tokens_to_ids('ben_Beng'),\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            num_beams=num_beams,\n",
    "                        )\n",
    "                        preds_bn = tokenizer.batch_decode(generated_bn, skip_special_tokens=True)\n",
    "                    \n",
    "                    print(f\"\\n{'='*70}\")\n",
    "                    print(f\"Step {global_step} | Training Caption Inspection\")\n",
    "                    print(f\"{'='*70}\")\n",
    "                    print(f\"True EN:       {captions[:2]}\")\n",
    "                    print(f\"Generated EN:  {preds_en[:2]}\")\n",
    "                    print(f\"Generated BEN: {preds_bn[:2]}\")\n",
    "                    print(f\"Loss: {loss.item() * grad_accum:.3f} | IMG vec std: {img_vec_eval.std():.4f}\")\n",
    "                    print(f\"{'='*70}\\n\")\n",
    "                    \n",
    "                    image_encoder.train()\n",
    "\n",
    "                if global_step % eval_every == 0:\n",
    "                    val_loss = validate(image_encoder, decoder_model, val_loader,\n",
    "                                        tokenizer, device, max_batches=30)\n",
    "                    print(f\"\\nStep {global_step}  val-loss {val_loss:.4f}\")\n",
    "                    if val_loss < best_val:\n",
    "                        best_val = val_loss\n",
    "                        torch.save({'step': global_step,\n",
    "                                    'encoder': image_encoder.state_dict(),\n",
    "                                    'opt': optimizer.state_dict()},\n",
    "                                   f\"{save_dir}/best.pt\")\n",
    "                if global_step >= max_steps:\n",
    "                    break\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nDone  best val-loss {best_val:.4f}\")\n",
    "    return global_step, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:33:36.163999Z",
     "iopub.status.busy": "2025-12-18T14:33:36.163782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "global_step, best_loss = train_multilingual(\n",
    "    image_encoder=image_encoder,\n",
    "    decoder_model=decoder_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    max_steps=12000,\n",
    "    grad_accum=10,\n",
    "    lr=5e-5,\n",
    "    eval_every=1000,\n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 285082234,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
